# Аналитика качества данных в небольшом DWH прототипе
# DWH_dota2_quality

# Обзор проекта
Проект реализует мини-хранилище данных (DWH) на основе открытых данных турниров Dota 2: The International (2011–2021).

Цель: построить воспроизводимую ETL‑цепочку, нормализовать данные, загрузить их в базу и создать аналитические витрины.

Проект организован по классической архитектуре Stage → DDS → DM.

# Источники данных

Данные были скачаны с открытого набора на Kaggle: "DOTA 2 The International Dataset (2011-2021)"

https://www.kaggle.com/datasets/arpan129/dota-2-the-international-complete-dataset/data

и сохранены в папку /data/raw/

Исходные наборы данных включают:

 • Результаты турниров (топ-3 команд и призовой фонд)
 
 • Страны игроков
 
 • Статистика пиков героев (разные форматы в разные годы)
 
 • Составы команд для каждого турнира
 
Каждый год представлен отдельным CSV, требующим согласования схем при ETL.

# Структура репозитория

/data

    /raw        # Сырые CSV с исходными данными
    
    /processed  # Очищенные CSV после ETL

/sql

    /stage      # Скрипты для создания таблиц слоя Stage
    
    /dds        # Скрипты для нормализованных таблиц DDS
    
    /dm         # Скрипты для фактов и измерений DM
    
requirements.txt  # Зависимости Python для воспроизводимого окружения


ETL_Dota2_TI.ipynb # ETL пайплайн

# ETL-процесс (Colab)
 1. Extract: загрузка CSV по годам турниров
    
 2. Transform: очистка данных, нормализация имен колонок, приведение типов

 3. DATA QUALITY VALIDATION: контроль качества данных

 4. Load: сохранение очищенных CSV в data/processed/


Все преобразования реализованы в одном воспроизводимом блокноте Google Colab. В Colab используется только pandas.

# Extract (Загрузка)

 • Генерация ссылок на CSV динамически по годам
 
 • Автоматическая загрузка всех источников
 
 • Метаданные фиксируют источник и год

 # Transform (Преобразование)
 
 • Нормализация схемы и названий колонок
 
 • Очистка числовых значений ($, %, запятые)
 
 • Разделение мульти-значных полей (игроки по странам)
 
 • Обогащение бизнес-правилами (роль игрока по позиции в команде)

 # Validate (Контроль качества данных)

Для всех подготовленных таблиц (winners_top3, country, hero_picks, teams) реализованы автоматические проверки качества данных:

 • Отсутствие пустых значений в ключевых колонках (например, year, team_name, player_name, hero).
 
 • Согласованность бизнес-правил:
 
   Для winners_top3 - допустимые значения мест (Place = 1st/2nd/3rd) и ровно 3 записи на год.
   
   Для teams - корректное назначение ролей игроков (player или other).
   
 • Гранулярность фактов: уникальные комбинации ключей на уровне фактов (например, year + Place для топ‑3 победителей).
 
 • Проверка на дубли - исключение повторяющихся строк после очистки.
 
 • Логическая согласованность данных - например, каждый год содержит корректное число записей в соответствии с правилами источника.

Пайплайн останавливается при нарушении правил качества, что обеспечивает безопасную загрузку данных в DWH.

# Load Preparation (Подготовка к загрузке)

Очищенные данные сохраняются в:

output_path = "/content/"

После выгружаются на компьютер и сохраняются в репозиторий в: /data/processed/

 В папке /data/processed/ появились файлы:
 
 • winners_top3.csv
 
 • country.csv
 
 • hero_picks.csv
 
 • teams.csv

# SQL-процесс и слои DWH

Локальная PostgreSQL

 Сервер: PostgreSQL, локально на компьютере
 
 Подключение через DBeaver:

db_user = 'postgres'

db_password = 'superkroshka1'

db_host = 'localhost'

db_port = '5432'

db_name = 'dota_dwh'

Назначение: хранение всех слоев DWH и построение витрин/фактов для аналитики

# Слои DWH и их создание

Проект реализован по классической архитектуре Stage → DDS → DM.

# Stage

Назначение: сырой слой, хранение данных из CSV без трансформаций

Файлы в репозитории:

 • sql/stage/create_stage_tables.sql — создание схемы и таблиц
 
 • sql/stage/load_stage.md — инструкция по загрузке данных из CSV (data/processed/*.csv) вручную через DBeaver

Очищенные CSV из ETL-пайплайна сохраняются в папку /data/processed/

Далее они загружаются в PostgreSQL локально на компьютере с помощью DBeaver.

Схема загрузки:

Таблица Stage → CSV-файл

stage.winners_raw winners_top3.csv

stage.teams_raw teams.csv

stage.country_raw country.csv

stage.hero_picks_raw hero_picks.csv

 • load_stage.md инструкция как данные загружались в Stage вручную из очищенных CSV

# DDS

Назначение: нормализованные таблицы измерений (dim_*), справочники
Файлы в репозитории:
 • sql/dds/create_dds_tables.sql — создание нормализованных таблиц
 • sql/dds/load_dds_table.sql — загрузка данных в DDS из Stage

Измерения (dim_*):

Таблица Ключ Атрибуты

dim_year year_id  - year

dim_team team_id - team_name

dim_player player_id - player_name

dim_country country_id - country

dim_hero hero_id - hero

# DM

Назначение: факты (fact_*) и аналитические витрины для построения дашбордов

Файлы в репозитории:

 • sql/dm/create_dm_tables.sql — создание таблиц фактов
 
 • sql/dm/load_dm_facts.sql — загрузка фактов из DDS
 
 • sql/dm/add_foreign_keys.sql — настройка внешних ключей и связей между таблицами

Факты (fact_*):

Таблица Гранулярность Ключи Меры / Атрибуты

f_winners_top3 одна строка на год и место year_id, team_id, place prize_usd, prize_percent

f_team_roster год, команда, игрок year_id, team_id, player_id role

f_hero_picks год и герой year_id, hero_id times_picked

f_player_country год и игрок year_id, player_id, country_id —

Пример аналитики:

Для расчета заработка по странам используется цепочка:

f_winners_top3 → f_team_roster → f_player_country → dim_country

То есть одна строка факта с топ‑3 победителем соединяется с игроками команды, затем с их странами, и агрегируется для отчета.

Воспроизводимость

 Все SQL-скрипты и инструкции хранятся в репозитории и позволяют полностью воспроизвести слои Stage, DDS и DM.
 
 CSV-файлы для Stage находятся в /data/processed/.
 
 Проверяющий может:
 
 1. Создать базу на локальном PostgreSQL

 2. Запустить SQL-скрипты по порядку: Stage → DDS → DM
   
 3. Проверить загрузку данных и связи между таблицами
 
