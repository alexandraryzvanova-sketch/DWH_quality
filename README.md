# Аналитика качества данных в небольшом DWH прототипе
# DWH_dota2_quality

# Обзор проекта
Проект реализует мини-хранилище данных (DWH) на основе открытых данных турниров Dota 2: The International (2011–2021).

Цель: построить воспроизводимую ETL‑цепочку, нормализовать данные, загрузить их в базу и создать аналитические витрины.

Проект организован по классической архитектуре Stage → DDS → DM.

# Источники данных

Данные были скачаны с открытого набора на Kaggle: "DOTA 2 The International Dataset (2011-2021)"

https://www.kaggle.com/datasets/arpan129/dota-2-the-international-complete-dataset/data

и сохранены в папку /data/raw/

Исходные наборы данных включают:

 • Результаты турниров (топ-3 команд и призовой фонд)
 
 • Страны игроков
 
 • Статистика пиков героев (разные форматы в разные годы)
 
 • Составы команд для каждого турнира
 
Каждый год представлен отдельным CSV, требующим согласования схем при ETL.

# Структура репозитория

/data

    /raw        # Сырые CSV с исходными данными
    
    /processed  # Очищенные CSV после ETL

/sql

    /stage      # Скрипты для создания таблиц слоя Stage
    
    /dds        # Скрипты для нормализованных таблиц DDS
    
    /dm         # Скрипты для фактов и измерений DM

    /marts     # Аналитические представления
    
requirements.txt  # Зависимости Python для воспроизводимого окружения


ETL_Dota2_TI.ipynb # ETL пайплайн

# ETL-процесс (Jupyter Notebook)

 1. Extract: загрузка CSV по годам турниров
    
 2. Transform: очистка данных, нормализация имен колонок, приведение типов

 3. DATA QUALITY: контроль качества данных

 4. Load: сохранение очищенных CSV в data/processed/

ETL реализован в Jupyter Notebook (разрабатывался в Google Colab, но может быть выполнен в любой среде с Python и pandas).

# Extract (Загрузка)

 • Генерация ссылок на CSV динамически по годам
 
 • Автоматическая загрузка всех источников
 
 • Метаданные фиксируют источник и год

 # Transform (Преобразование)
 
 • Нормализация схемы и названий колонок
 
 • Очистка числовых значений ($, %, запятые)
 
 • Разделение мульти-значных полей (игроки по странам)
 
 • Обогащение бизнес-правилами (роль игрока по позиции в команде)

 # DATA QUALITY (Контроль качества данных)

Для всех подготовленных таблиц (winners_top3, country, hero_picks, teams) реализованы автоматические проверки качества данных:

 • Отсутствие пустых значений в ключевых колонках (например, year, team_name, player_name, hero).
 
 • Согласованность бизнес-правил:
 
   Для winners_top3 - допустимые значения мест (Place = 1st/2nd/3rd) и ровно 3 записи на год.
   
   Для teams - корректное назначение ролей игроков (player или other).
   
 • Гранулярность фактов: уникальные комбинации ключей на уровне фактов (например, year + Place для топ‑3 победителей).
 
 • Проверка на дубли - исключение повторяющихся строк после очистки.
 
 • Логическая согласованность данных - проверяется ожидаемое количество записей на каждый год

Пайплайн останавливается при нарушении правил качества, что обеспечивает безопасную загрузку данных в DWH.

# Load Preparation (Подготовка к загрузке)

Очищенные данные сохраняются в:

output_path = "/content/"

После выгружаются на компьютер и сохраняются в репозиторий в: /data/processed/

 В папке /data/processed/ появились файлы:
 
 • winners_top3.csv
 
 • country.csv
 
 • hero_picks.csv
 
 • teams.csv

# SQL-процесс и слои DWH

Локальная PostgreSQL

 Сервер: PostgreSQL, локально на компьютере
 
 Подключение через DBeaver:

db_user = 'postgres'

db_password = 'superkroshka1'

db_host = 'localhost'

db_port = '5432'

db_name = 'dota_dwh'

Назначение: хранение всех слоев DWH и построение витрин/фактов для аналитики

# Слои DWH и их создание

Проект реализован по классической архитектуре Stage → DDS → DM → Marts

# Stage

Назначение: сырой слой, содержит данные после первичной очистки в ETL, но до моделирования и нормализации в DDS, используется для изоляции источника от DWH-модели

Файлы в репозитории:

 • sql/stage/create_stage_tables.sql — создание схемы и таблиц
 
 • sql/stage/load_stage.md — инструкция по загрузке данных из CSV (data/processed/*.csv) вручную через DBeaver

Очищенные CSV из ETL-пайплайна сохраняются в папку /data/processed/

Далее они загружаются в PostgreSQL локально на компьютере с помощью DBeaver.

Схема загрузки:

Таблица Stage → CSV-файл

stage.winners_raw winners_top3.csv

stage.teams_raw teams.csv

stage.country_raw country.csv

stage.hero_picks_raw hero_picks.csv

 • load_stage.md инструкция как данные загружались в Stage вручную из очищенных CSV

# DDS

Назначение: нормализованные таблицы измерений (dim_*), справочники

Файлы в репозитории:

 • sql/dds/create_dds_tables.sql — создание нормализованных таблиц
 
 • sql/dds/load_dds_table.sql — загрузка данных в DDS из Stage

Измерения (dim_*):

Таблица Ключ Атрибуты

dim_year year_id  - year

dim_team team_id - team_name

dim_player player_id - player_name

dim_country country_id - country

dim_hero hero_id - hero

# DM

Назначение: факты (fact_*) и аналитические витрины для построения дашбордов

Файлы в репозитории:

 • sql/dm/create_dm_tables.sql — создание таблиц фактов
 
 • sql/dm/load_dm_facts.sql — загрузка фактов из DDS
 
 • sql/dm/add_foreign_keys.sql — настройка внешних ключей и связей между таблицами

Факты (fact_*):

Таблица Гранулярность Ключи Меры / Атрибуты

f_winners_top3 одна строка на год и место year_id, team_id, place prize_usd, prize_percent

f_team_roster год, команда, игрок year_id, team_id, player_id role

f_hero_picks год и герой year_id, hero_id times_picked

f_player_country год и игрок year_id, player_id, country_id

Каждая факт-таблица имеет собственную бизнес-гранулярность, что позволяет анализировать разные аспекты турниров без денормализации модели.

# Пример аналитики:

Для расчета заработка по странам используется цепочка:

f_winners_top3 → f_team_roster → f_player_country → dim_country

То есть одна строка факта с топ‑3 победителем соединяется с игроками команды, затем с их странами, и агрегируется для отчета.

# Marts

Папка /sql/marts содержит SQL-запросы для формирования аналитических витрин (Data Marts).
Это финальный слой хранилища, предназначенный не для загрузки данных, а для их анализа и
визуализации в дашбордах.

Витрины строятся на основе схемы dm, где уже находятся согласованные факты и измерения.
Каждый файл в папке - это воспроизводимый SQL-запрос, отвечающий на конкретный бизнес-вопрос.

# Реализованные витрины

mart_team_prize.sql - Анализ призовых по командам.

Гранулярность: год – команда
 
Метрики:

 total_prize_usd - сумма выигранных призовых
 
 Используется для:
 
 динамики заработка команд по годам;
 
 сравнения успешности команд.

⸻

mart_player_prize.sql - Оценка призовых игроков (с допущением равного деления командного выигрыша)

 Допущение: приз команды делится поровну между 5 игроками основного состава.
 
 Гранулярность: год – игрок
 
 Метрики:
 
 player_prize_usd — расчётный доход игрока
 
 Позволяет анализировать:
 
  самых успешных игроков;
  
  распределение доходов по странам.

⸻

mart_team_top3_frequency.sql - Частота попадания команд в топ-3.

 Гранулярность: команда
 
 Метрика:
 
 top3_count — количество попаданий в призовые места.
 
 Используется для:
 
  оценки стабильности команд;
  
  построения рейтингов.

⸻

mart_hero_popularity.sql - Популярность героев по годам.

 Гранулярность: год – герой
 
 Метрика:
 
 times_picked — количество выборов героя.
 
 Позволяет анализировать:
 
  мету игры;
  
  тренды популярности персонажей.

# Как используются витрины

 1. SQL выполняется локально в PostgreSQL (через DBeaver).
  
 2. Результат запроса экспортируется в CSV.
  
 3. CSV используется в BI-инструменте (Yandex DataLens) для построения дашбордов.

Такой подход выбран, потому что база данных развёрнута локально и не публикуется в облаке,
что соответствует формату учебного проекта.


Таким образом осуществяется разделение на слои:

/sql/stage   → загрузка сырых данных

/sql/dds     → нормализация и очистка

/sql/dm      → факты и измерения (схема созвездие /fact constellation)

/sql/marts   → аналитические представления

В модели используется схема созвездия фактов: несколько факт-таблиц описывают разные бизнес-процессы (результаты турниров, составы команд, пики героев, принадлежность игроков к странам) и разделяют согласованные измерения (год, команда, игрок, герой, страна).
 
# Воспроизводимость

 Все SQL-скрипты и инструкции хранятся в репозитории и позволяют полностью воспроизвести слои Stage, DDS и DM.
 
 CSV-файлы для Stage находятся в /data/processed/.
 
 1. Установить PostgreSQL локально.
   
 2. Создать пустую БД:

CREATE DATABASE dota_dwh;

 3. Выполнить скрипты по порядку:

sql/stage/create_stage_tables.sql 

загрузить CSV по инструкции sql/stage/load_stage.md

sql/dds/create_dds_tables.sql

sql/dds/load_dds_table.sql

sql/dm/create_dm_tables.sql

sql/dm/load_dm_facts.sql

sql/dm/add_foreign_keys.sql

 4. Выполнить запросы из /sql/marts
    
 5. Экспортировать результаты в CSV для визуализации.
