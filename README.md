# Аналитика качества данных в небольшом DWH прототипе
# DWH_dota2_quality

# Обзор проекта
Проект реализует мини-хранилище данных (DWH) на основе открытых данных турниров Dota 2: The International (2011–2021).

Цель: построить воспроизводимую ETL‑цепочку, нормализовать данные, загрузить их в базу и создать аналитические витрины.

Проект организован по классической архитектуре Stage → DDS → DM.

# Источники данных

Данные были скачаны с открытого набора на Kaggle: "DOTA 2 The International Dataset (2011-2021)"

https://www.kaggle.com/datasets/arpan129/dota-2-the-international-complete-dataset/data

и сохранены в папку /data/raw/

Исходные наборы данных включают:

 • Результаты турниров (топ-3 команд и призовой фонд)
 
 • Страны игроков
 
 • Статистика пиков героев (разные форматы в разные годы)
 
 • Составы команд для каждого турнира
 
Каждый год представлен отдельным CSV, требующим согласования схем при ETL.

# Структура репозитория

/data

    /raw        # Сырые CSV с исходными данными
    
    /processed  # Очищенные CSV после ETL

/sql

    /stage      # Скрипты для создания таблиц слоя Stage
    
    /dds        # Скрипты для нормализованных таблиц DDS
    
    /dm         # Скрипты для фактов и измерений DM
    
requirements.txt  # Зависимости Python для воспроизводимого окружения

ETL_Dota2_TI.ipynb # ETL пайплайн

# ETL-процесс (Colab)
 1. Extract — загрузка CSV по годам турниров
    
 2. Transform — очистка данных, нормализация имен колонок, приведение типов

 3. DATA QUALITY VALIDATION - не знаю что ту написать

 4. Load — сохранение очищенных CSV в data/processed/


Все преобразования реализованы в одном воспроизводимом блокноте Google Colab.

# Extract (Загрузка)

 • Генерация ссылок на CSV динамически по годам
 
 • Автоматическая загрузка всех источников
 
 • Метаданные фиксируют источник и год

 # Transform (Преобразование)
 
 • Нормализация схемы и названий колонок
 
 • Очистка числовых значений ($, %, запятые)
 
 • Разделение мульти-значных полей (игроки по странам)
 
 • Обогащение бизнес-правилами (роль игрока по позиции в команде)

 # Validate (Контроль качества данных)

Внедрены автоматические проверки:

 • Нет пустых значений в ключевых колонках
 
 • Допустимые значения мест (Place = 1st/2nd/3rd)
 
 • Гранулярность фактов (один год = 3 записи топ-3)
 
 • Проверка на дубли
 
 • Логическая согласованность (каждый год содержит ровно топ-3)

Пайплайн останавливается при нарушении правил качества.

# Load Preparation (Подготовка к загрузке)

Очищенные данные сохраняются в:

output_path = "/content/"

После выгружаются на компьютер и сохраняются в репозиторий в: /data/processed/

 В папке /data/processed/ появились файлы:
 
 • winners_top3.csv
 
 • country.csv
 
 • hero_picks.csv
 
 • teams.csv

