# Аналитика качества данных в небольшом DWH прототипе
# DWH_dota2_quality

# Обзор проекта
Проект реализует мини-хранилище данных (DWH) на основе открытых данных турниров Dota 2: The International (2011–2021).

Цель: построить воспроизводимую ETL‑цепочку, нормализовать данные, загрузить их в базу и создать аналитические витрины.

Проект организован по классической архитектуре Stage → DDS → DM.

# Источники данных

Данные были скачаны с открытого набора на Kaggle: "DOTA 2 The International Dataset (2011-2021)"

https://www.kaggle.com/datasets/arpan129/dota-2-the-international-complete-dataset/data

и сохранены в папку /data/raw/

Исходные наборы данных включают:

 • Результаты турниров (топ-3 команд и призовой фонд)
 
 • Страны игроков
 
 • Статистика пиков героев (разные форматы в разные годы)
 
 • Составы команд для каждого турнира
 
Каждый год представлен отдельным CSV, требующим согласования схем при ETL.

# Структура репозитория

/data

    /raw        # Сырые CSV с исходными данными
    
    /processed  # Очищенные CSV после ETL

/sql

    /stage      # Скрипты для создания таблиц слоя Stage
    
    /dds        # Скрипты для нормализованных таблиц DDS
    
    /dm         # Скрипты для фактов и измерений DM
    
requirements.txt  # Зависимости Python для воспроизводимого окружения


ETL_Dota2_TI.ipynb # ETL пайплайн

# ETL-процесс (Colab)
 1. Extract: загрузка CSV по годам турниров
    
 2. Transform: очистка данных, нормализация имен колонок, приведение типов

 3. DATA QUALITY VALIDATION: контроль качества данных

 4. Load: сохранение очищенных CSV в data/processed/


Все преобразования реализованы в одном воспроизводимом блокноте Google Colab. В Colab используется только pandas.

# Extract (Загрузка)

 • Генерация ссылок на CSV динамически по годам
 
 • Автоматическая загрузка всех источников
 
 • Метаданные фиксируют источник и год

 # Transform (Преобразование)
 
 • Нормализация схемы и названий колонок
 
 • Очистка числовых значений ($, %, запятые)
 
 • Разделение мульти-значных полей (игроки по странам)
 
 • Обогащение бизнес-правилами (роль игрока по позиции в команде)

 # Validate (Контроль качества данных)

Для всех подготовленных таблиц (winners_top3, country, hero_picks, teams) реализованы автоматические проверки качества данных:

 • Отсутствие пустых значений в ключевых колонках (например, year, team_name, player_name, hero).
 
 • Согласованность бизнес-правил:
 
   Для winners_top3 - допустимые значения мест (Place = 1st/2nd/3rd) и ровно 3 записи на год.
   
   Для teams - корректное назначение ролей игроков (player или other).
   
 • Гранулярность фактов: уникальные комбинации ключей на уровне фактов (например, year + Place для топ‑3 победителей).
 
 • Проверка на дубли - исключение повторяющихся строк после очистки.
 
 • Логическая согласованность данных - например, каждый год содержит корректное число записей в соответствии с правилами источника.

Пайплайн останавливается при нарушении правил качества, что обеспечивает безопасную загрузку данных в DWH.

# Load Preparation (Подготовка к загрузке)

Очищенные данные сохраняются в:

output_path = "/content/"

После выгружаются на компьютер и сохраняются в репозиторий в: /data/processed/

 В папке /data/processed/ появились файлы:
 
 • winners_top3.csv
 
 • country.csv
 
 • hero_picks.csv
 
 • teams.csv

